import random
import os
from collections import defaultdict, deque
import numpy as np
import matplotlib as mpl
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
import csv
# import time as ti
# import pandas as pd

def update_Q(Qsa, Qsa_next, reward, alpha = 0.01, gamma = 1.0):
	""" updates the action-value function estimate using the most recent time step """
	return Qsa + (alpha * (reward + (gamma * Qsa_next) - Qsa))

def epsilon_greedy_probs( Q_s, i_episode, eps = None):
	""" obtains the action probabilities corresponding to epsilon-greedy policy """
	epsilon = 1.0 / (i_episode+1)
	if eps is not None:
		epsilon = eps
	policy_s = np.ones(a_size) * epsilon / a_size
	policy_s[np.argmax(Q_s)] = 1 - epsilon + (epsilon / a_size)
	return abs(policy_s)

def reward_f(d):
	latency = float(d["average_packet_latency"][-1])
	return -round(latency,2) # minimize latency

def ICN_env(injrate, action): # injrate = state
	os_command = "./build/Garnet_standalone/gem5.opt configs/example/garnet_synth_traffic.py --network=garnet2.0 --num-cpus=64 --num-dirs=64 --topology=Mesh --mesh-rows=8 --sim-cycles=20000 --inj-vnet=0 --injectionrate={:f} --synthetic=shuffle --routing-algorithm={}".format(injrate, action)
	os.system(os_command)
	os_command2 = "./my_scripts/extract_network_stats.sh"
	os.system(os_command2)
	with open("./network_stats.txt", "r") as fd:
		for line in fd:
			line_ele = line.split(" ")
			if len(line_ele) > 3:
				my_line = line_ele
				key = my_line[0]
				val = my_line[2]
				dicts[key].append(val)
	# print(dicts)

	return dicts

## Global Parameters
actions = ["xy", "random_oblivious", "turn_model_oblivious", "turn_model_adaptive"]
a_size = len(actions) # space size of action
Q = defaultdict(lambda: np.zeros(a_size)) # Q-Table
dicts = defaultdict(list)
action_index = random.randint(0, 100)%2
action = actions[action_index]
iter_step = 6 # injection from 0.1 to 0.6
total_episodes = 1 # Game Playing times

epsilon = 1.0       # exploration rate
eps_min = 0.01
eps_decay = 0.999

### Plot Notebooks
time_history = []
rew_history = []
Q = defaultdict(lambda: np.zeros(a_size))

state = 0.1 # = Injection_rate as reset state env.reset()
# dicts = ICN_env(state, action) # ICM simulate()
for i in range(iter_step):
	state = (i+1)/10 # get next state
	action = "xy"
	dicts = ICN_env(state, action)
	
	# action = actions[random.randint(0, 100)%2]

rew_history.append(0) # Recording rewards



print('Q-Table = ', Q)

print('Reward = ', rew_history)

# print('Dicts = ',dicts)

csv_columns = ['average_flit_latency','average_packet_queueing_latency','average_flit_network_latency','average_flit_queueing_latency','packets_injected', 'average_packet_network_latency', 'average_hops',  'flits_injected', 'packets_received',  'flits_received', 'average_packet_latency']
csv_file = 'Inter_Connect_Networks/Tables/env_base_'+str(iter_step)+'_' +str(total_episodes)+ '.csv'

try:
	with open(csv_file, 'w', newline='') as csvfile:
		writer = csv.writer(csvfile)
		writer.writerow(csv_columns)
		for i in range(len(dicts['average_flit_latency'])):
			writer.writerow([dicts[key][i] for key in csv_columns])
except IOError:
    print("I/O error")
# np.savetxt("Reward_history.csv", rew_history, delimiter=",")
### Plotting 

# print("Learning Performance")
mpl.rcdefaults()
mpl.rcParams.update({'font.size': 16})

fig, ax = plt.subplots(figsize=(10,4))
# plt.grid(True, linestyle='--')
plt.title('ICNs Learning')
# plt.plot(range(len(time_history)), time_history, label='Steps', marker="^", linestyle=":")#, color='red')
plt.plot(range(len(rew_history)), rew_history, label='Reward', marker="", linestyle="-")#, color='k')
plt.xlabel('Episodes')
plt.ylabel('Reward')
plt.savefig('Inter_Connect_Networks/Figures/shuffle_SARSA_'+str(iter_step)+'_'+str(total_episodes)+'_ICN.png', bbox_inches='tight')